{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e35c67",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:15.4321702Z",
       "execution_start_time": "2025-05-14T20:19:14.9023447Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "89553287-3244-4a36-9803-b30207c1bb7b",
       "queued_time": "2025-05-14T20:19:14.9011638Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================\n",
    "# Setup: Imports & Configuration\n",
    "# =======================================\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Spark libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ADME Ingestion\").getOrCreate()\n",
    "\n",
    "# ========== USER CONFIGURATION ==========\n",
    "TENANT_ID = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
    "CLIENT_ID = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n",
    "CLIENT_SECRET = \"xxxxxxxxxx\"\n",
    "RESOURCE = \"https://<your_adme_instance>.energy.azure.com\"\n",
    "PARTITION_ID = \"xxxxx\"\n",
    "BASE_URL = f\"{RESOURCE}/api\"\n",
    "SCOPE = f\"{CLIENT_ID}/.default\"\n",
    "\n",
    "LEGAL_TAG = \"xxxxx\"\n",
    "DATA_ROOT = \"/lakehouse/default/Files/nz-data-pack-small\"\n",
    "REFERENCE_DATA_FOLDER = f\"{DATA_ROOT}/reference-data\"\n",
    "MASTER_DATA_WELL = f\"{DATA_ROOT}/master-data/Well.1.3.0.json\"\n",
    "MASTER_DATA_WELLBORE = f\"{DATA_ROOT}/master-data/Wellbore.1.5.0.json\"\n",
    "METADATA_WELLLOG = f\"{DATA_ROOT}/work-product-components/WellLog.1.4.0.json\"\n",
    "METADATA_MARKERSET = f\"{DATA_ROOT}/work-product-components/WellboreMarkerSet.1.4.0.json\"\n",
    "METADATA_TRAJECTORY = f\"{DATA_ROOT}/work-product-components/WellboreTrajectory.1.3.0.json\"\n",
    "PARQUET_LOGS_FOLDER = f\"{DATA_ROOT}/work-product-components/logs\"\n",
    "PARQUET_TRAJECTORIES_FOLDER = f\"{DATA_ROOT}/work-product-components/trajectories\"\n",
    "\n",
    "# Construct endpoint URLs\n",
    "STORAGE_URL = f\"{BASE_URL}/storage/v2/records\"\n",
    "WELLLOG_DMS_URL = f\"{BASE_URL}/os-wellbore-ddms/ddms/v3/welllogs\"\n",
    "WELL_DMS_URL = f\"{BASE_URL}/os-wellbore-ddms/ddms/v3/wells\"\n",
    "WELLBORE_DMS_URL = f\"{BASE_URL}/os-wellbore-ddms/ddms/v3/wellbores\"\n",
    "TRAJECTORY_DMS_URL = f\"{BASE_URL}/os-wellbore-ddms/ddms/v3/wellboretrajectories\"\n",
    "MARKERSET_DMS_URL = f\"{BASE_URL}/os-wellbore-ddms/ddms/v3/wellboremarkersets\"\n",
    "\n",
    "headers = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5667e8a-f28f-4479-a5bf-cc8fee6ab3da",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:15.759836Z",
       "execution_start_time": "2025-05-14T20:19:15.4342435Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "379e0f09-0e2b-458f-a9d1-52f578f21b10",
       "queued_time": "2025-05-14T20:19:14.969699Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 16, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================\n",
    "# Authentication\n",
    "# =======================================\n",
    "def get_adme_token(tenant_id, client_id, client_secret, scope):\n",
    "    try:\n",
    "        url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n",
    "        payload = {\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": client_id,\n",
    "            \"client_secret\": client_secret,\n",
    "            \"scope\": scope\n",
    "        }\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "        }\n",
    "        response = requests.post(url, data=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"access_token\"]\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to get token: {str(e)}\")\n",
    "\n",
    "access_token = get_adme_token(TENANT_ID, CLIENT_ID, CLIENT_SECRET, SCOPE)\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"data-partition-id\": PARTITION_ID,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "#print(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35812956",
   "metadata": {
    "editable": true,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:16.0884871Z",
       "execution_start_time": "2025-05-14T20:19:15.7619621Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e5bf399d-8a24-4948-9f32-8690c3e9f778",
       "queued_time": "2025-05-14T20:19:15.0099901Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 17,
       "statement_ids": [
        17
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 17, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================\n",
    "# Ingest Reference Data by file\n",
    "# =======================================\n",
    "def ingest_reference_data_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            records = json.load(f)\n",
    "        for record in records:\n",
    "            response = requests.post(STORAGE_URL, headers=headers, json=record)\n",
    "            if response.status_code not in [200, 201, 409]:\n",
    "                print(f\"Failed to ingest reference record: {response.text}\")\n",
    "        print(f\"Ingested reference data from {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Reference Data Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "286f34b0-def5-4f3f-8867-2218455e70ef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:16.402211Z",
       "execution_start_time": "2025-05-14T20:19:16.0904542Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6a90b914-c359-4bea-bb88-05f18a6b35b1",
       "queued_time": "2025-05-14T20:19:15.1303305Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 18,
       "statement_ids": [
        18
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 18, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================\n",
    "# Ingest Reference Data\n",
    "# =======================================\n",
    "def ingest_reference_data_folder(directory_path):\n",
    "    try:\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if os.path.isfile(os.path.join(directory_path, filename)) and filename.endswith('.json'):\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    records = json.load(f)\n",
    "                # Send the entire list of records at once\n",
    "                response = requests.put(STORAGE_URL, headers=headers, data=json.dumps(records))\n",
    "                if response.status_code not in [200, 201, 409]:\n",
    "                    print(f\"Failed to ingest reference data from {file_path}: {response.text}\")\n",
    "                else:\n",
    "                    print(f\"Ingested reference data from {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Reference Data Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5685650",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:16.7347746Z",
       "execution_start_time": "2025-05-14T20:19:16.4042257Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c48de160-c785-4475-9288-3d43deb92a96",
       "queued_time": "2025-05-14T20:19:15.1998183Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 19,
       "statement_ids": [
        19
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 19, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================\n",
    "# Ingest Master Data: Well & Wellbore\n",
    "# =======================================\n",
    "def ingest_master_data(file_path, url, label):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            record = json.load(f)\n",
    "        response = requests.post(url, headers=headers, json=record)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Ingested {label} data successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Master Data Error ({label}): {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "012d5070-3d01-4f09-a4ff-40180847a4f9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:17.0800473Z",
       "execution_start_time": "2025-05-14T20:19:16.7366948Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "37e739ae-db8b-43df-9093-204fe003510f",
       "queued_time": "2025-05-14T20:19:15.2665492Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 20,
       "statement_ids": [
        20
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 20, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================\n",
    "# Ingest Meta Data\n",
    "# =======================================\n",
    "def ingest_metadata(file_path, url, label):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            records = json.load(f)  # Load the full JSON array\n",
    "\n",
    "        for record in records:\n",
    "            # Wrap the record in a list to match the expected payload\n",
    "            response = requests.post(url, headers=headers, json=[record])\n",
    "            if response.status_code in [200, 201, 409]:\n",
    "                record_id = response.json()[\"recordIds\"][0]\n",
    "                print(f\"Ingested {label} record with ID: {record_id}\")\n",
    "            else:\n",
    "                print(f\"Failed to ingest {label} record: {response.text}\")\n",
    "\n",
    "        print(f\"Finished ingesting {label} data from {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{label} Ingestion Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "868e694e-4c62-4122-9c92-ce84ac835d7a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:17.4099348Z",
       "execution_start_time": "2025-05-14T20:19:17.0821314Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a514a195-6c6f-43c5-ab1d-af0b3126db85",
       "queued_time": "2025-05-14T20:19:15.3364466Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 21,
       "statement_ids": [
        21
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 21, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================\n",
    "# Helper functions to get and delete records\n",
    "# =======================================\n",
    "\n",
    "def get_record(url, record_id):\n",
    "    url = f\"{url}/{record_id}\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred while getting record {record_id}: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Unexpected error occurred while getting record {record_id}: {err}\")\n",
    "    return None\n",
    "\n",
    "def delete_record(url, record_id):\n",
    "    url = f\"{url}/{record_id}\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    try:\n",
    "        response = requests.delete(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred while deleting record {record_id}: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Unexpected error occurred while deleting record {record_id}: {err}\")\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a55e6c26-649a-44af-be75-bc940ed7fadf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:17.7570854Z",
       "execution_start_time": "2025-05-14T20:19:17.4118811Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "703da170-b3f1-42ea-bc1f-1014f95b6f51",
       "queued_time": "2025-05-14T20:19:15.3943216Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 22,
       "statement_ids": [
        22
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 22, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================\n",
    "# Ingest Meta Data by handling bulkURI if it already exist [Provided for convenience, not used in this example]\n",
    "# 'bulkURI' is added when you ingest data for the first time by OSDU, if re-ingesting, \n",
    "# ensure to remove the record before adding it again\n",
    "# =======================================\n",
    "def ingest_metadata_handle_bulk_uri(file_path, url, label):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            records = json.load(f)\n",
    "\n",
    "        key_to_remove = \"ExtensionProperties\"\n",
    "        record_ids = []\n",
    "\n",
    "        for record in records:\n",
    "            # Remove ExtensionProperties if it exists\n",
    "            if key_to_remove in record.get(\"data\", {}):\n",
    "                record[\"data\"].pop(key_to_remove)\n",
    "\n",
    "            record_id = record[\"id\"]\n",
    "            existing = get_record(url, record_id)\n",
    "\n",
    "            if existing.status_code == 200:\n",
    "                try:\n",
    "                    # Check if bulkURI exists in the existing record\n",
    "                    bulk_uri = existing.json()[\"data\"][\"ExtensionProperties\"][\"wdms\"][\"bulkURI\"]\n",
    "                    print(f\"Existing record has bulkURI: {bulk_uri}\")\n",
    "                    print(\"Deleting and recreating the record...\")\n",
    "                    delete_record(url, record_id)\n",
    "                    response = requests.post(url, headers=headers, json=[record])\n",
    "                except KeyError:\n",
    "                    print(\"No bulkURI found, updating record without delete.\")\n",
    "                    response = requests.post(url, headers=headers, json=[record])\n",
    "            else:\n",
    "                response = requests.post(url, headers=headers, json=[record])\n",
    "\n",
    "            if response.status_code in [200, 201, 409]:\n",
    "                new_record_id = response.json()[\"recordIds\"][0]\n",
    "                print(f\"Ingested {label} record with ID: {new_record_id}\")\n",
    "                record_ids.append(new_record_id)\n",
    "            else:\n",
    "                print(f\"Failed to ingest {label} record: {response.text}\")\n",
    "\n",
    "        print(f\"Finished ingesting {label} data from {file_path}\")\n",
    "        return record_ids\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{label} Ingestion Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0602152d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:18.075852Z",
       "execution_start_time": "2025-05-14T20:19:17.7591109Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "32114b78-fc9a-43b3-9af9-6979728c9617",
       "queued_time": "2025-05-14T20:19:15.4709933Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 23,
       "statement_ids": [
        23
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 23, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# =======================================\n",
    "# Ingest Parquet Time Series for Well Logs\n",
    "# =======================================\n",
    "import glob\n",
    "\n",
    "def ingest_welllog_parquet_data(welllog_json_path, parquet_folder):\n",
    "    try:\n",
    "        with open(welllog_json_path, \"r\") as f:\n",
    "            records = json.load(f)\n",
    "\n",
    "        for record in records:\n",
    "            log_id_full = record.get(\"data\", {}).get(\"Id\")\n",
    "            if not log_id_full or \":\" not in log_id_full:\n",
    "                print(\"Skipping record with missing or malformed ID.\")\n",
    "                continue\n",
    "\n",
    "            log_id = log_id_full.split(\":\")[-1]\n",
    "            parquet_path = os.path.join(parquet_folder, f\"{log_id}.parquet\")\n",
    "\n",
    "            if not os.path.exists(parquet_path):\n",
    "                print(f\"Parquet file not found for log ID: {log_id}\")\n",
    "                continue\n",
    "\n",
    "            with open(parquet_path, \"rb\") as f:\n",
    "                file_bytes = f.read()\n",
    "\n",
    "            upload_url = f\"{WELLLOG_DMS_URL}/{log_id}/data\"\n",
    "            headers_bin = headers.copy()\n",
    "            headers_bin[\"Content-Type\"] = \"application/octet-stream\"\n",
    "\n",
    "            response = requests.post(upload_url, headers=headers_bin, data=file_bytes)\n",
    "            if response.status_code in [200, 201]:\n",
    "                print(f\"Successfully uploaded parquet for log ID: {log_id}\")\n",
    "            else:\n",
    "                print(f\"Failed to upload parquet for {log_id}: {response.status_code} - {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Parquet Ingestion Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52fcc2e9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T20:19:18.3969474Z",
       "execution_start_time": "2025-05-14T20:19:18.0780624Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "68f05044-58dd-4b04-8ac9-93d4a3ccf042",
       "queued_time": "2025-05-14T20:19:15.5353302Z",
       "session_id": "9ff45185-ec9c-4107-a1a2-7a048ed9bf7d",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 24,
       "statement_ids": [
        24
       ]
      },
      "text/plain": [
       "StatementMeta(, 9ff45185-ec9c-4107-a1a2-7a048ed9bf7d, 24, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# =======================================\n",
    "# Ingest Parquet Time Series for Trajectories\n",
    "# =======================================\n",
    "def ingest_trajectory_parquet_data(trajectory_json_path, parquet_folder):\n",
    "    try:\n",
    "        with open(trajectory_json_path, \"r\") as f:\n",
    "            records = json.load(f)\n",
    "\n",
    "        for record in records:\n",
    "            traj_id_full = record.get(\"data\", {}).get(\"Id\")\n",
    "            if not traj_id_full or \":\" not in traj_id_full:\n",
    "                print(\"Skipping record with missing or malformed ID.\")\n",
    "                continue\n",
    "\n",
    "            traj_id = traj_id_full.split(\":\")[-1]\n",
    "            parquet_path = os.path.join(parquet_folder, f\"{traj_id}.parquet\")\n",
    "\n",
    "            if not os.path.exists(parquet_path):\n",
    "                print(f\"Parquet file not found for trajectory ID: {traj_id}\")\n",
    "                continue\n",
    "\n",
    "            with open(parquet_path, \"rb\") as f:\n",
    "                file_bytes = f.read()\n",
    "\n",
    "            upload_url = f\"{TRAJECTORY_DMS_URL}/{traj_id}/data\"\n",
    "            headers_bin = headers.copy()\n",
    "            headers_bin[\"Content-Type\"] = \"application/octet-stream\"\n",
    "\n",
    "            response = requests.post(upload_url, headers=headers_bin, data=file_bytes)\n",
    "            if response.status_code in [200, 201]:\n",
    "                print(f\"Successfully uploaded parquet for trajectory ID: {traj_id}\")\n",
    "            else:\n",
    "                print(f\"Failed to upload parquet for {traj_id}: {response.status_code} - {response.text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trajectory Parquet Ingestion Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb7c26",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =======================================\n",
    "# FINAL INGESTION PIPELINE\n",
    "# =======================================\n",
    "print(\"Starting ADME Ingestion Workflow...\")\n",
    "\n",
    "# Reference Data\n",
    "ingest_reference_data_folder(REFERENCE_DATA_FOLDER)\n",
    "\n",
    "# Master Data\n",
    "ingest_master_data(MASTER_DATA_WELL, WELL_DMS_URL, \"Well\")\n",
    "ingest_master_data(MASTER_DATA_WELLBORE, WELLBORE_DMS_URL, \"Wellbore\")\n",
    "\n",
    "# Time Series Metadata\n",
    "ingest_metadata(METADATA_WELLLOG, WELLLOG_DMS_URL, \"Well Log\")\n",
    "ingest_metadata(METADATA_MARKERSET, MARKERSET_DMS_URL, \"Wellbore Marker Set\")\n",
    "ingest_metadata(METADATA_TRAJECTORY, TRAJECTORY_DMS_URL, \"Trajectory\")\n",
    "\n",
    "# Parquet Uploads\n",
    "ingest_welllog_parquet_data(\n",
    "    welllog_json_path=METADATA_WELLLOG,\n",
    "    parquet_folder=PARQUET_LOGS_FOLDER\n",
    ")\n",
    "\n",
    "ingest_trajectory_parquet_data(\n",
    "    trajectory_json_path=METADATA_TRAJECTORY,\n",
    "    parquet_folder=PARQUET_TRAJECTORIES_FOLDER\n",
    ")\n",
    "\n",
    "print(\"ADME Ingestion Workflow Complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "a454634b-f617-4582-bed2-48336fb04f28",
    "default_lakehouse_name": "adme_data",
    "default_lakehouse_workspace_id": "6f64157c-cd3d-4ce3-9cca-3e74fb2c367f",
    "known_lakehouses": [
     {
      "id": "a454634b-f617-4582-bed2-48336fb04f28"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
